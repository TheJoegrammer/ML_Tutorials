{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification using Naive Bayes\n",
    "\n",
    "## 1.1) Naive Bayes\n",
    "- Like Bayes, maps probabilities of observed input features given belonging classes, to probability distribution over classses based on Bayes' Theorem.\n",
    "    - \\begin{equation*}\n",
    "    P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n",
    "    \\end{equation*}\n",
    "  \n",
    "## 1.2) Naive Bayes Terms\n",
    "<p>\n",
    "-Naive Bayes is useful because it provides a very simple way to classify data into classes.<br>\n",
    " </p>\n",
    " <p>\n",
    "    -Given data sample x with n features (x1,x2,...,xn), we treat x like a feature vector.<br>\n",
    "    -Goal of Naive Bayes is to determine probabilities that sample belongs to each of K possible classes (y1,y2...yk).\n",
    " </p>\n",
    "    \\begin{equation*}\n",
    "    P(Yk|x)=\\frac{P(x|Yk)P(Yk)}{P(x)}\n",
    "    \\end{equation*}\n",
    "    \n",
    "         - P(Yk): Potrays how classes are distributed, providing no further knowledge from data taken in.\n",
    "             - \"What's the probability that this thing is the specific class Yk?\"\n",
    "         - P(Yk|x): Potrays how classes are distributed, provided with the extra knowledge of the observation.\n",
    "             - \"What's the probability that this thing is specific class Yk, given all these features are present?\"\n",
    "         - P(x|Yk): Joint distribution of n features given sample belonging to class Yk.\n",
    "             - \"What's the probability that all these features are present given that this thing is a specific class Yk?\"\n",
    "             - Because we (NAIVELY) assume feature independence, joint conditional distribution of n features is the joint product of individal feature conditional distributions.\n",
    "                 - P(x|Yk)=P(x1|yk)*P(x2|yk)*...*P(xn|yk).\n",
    "         - P(x): Evidence, solely depending on the distribution of features not specific to certain classes.\n",
    "             - \"What's the probability that all these features are present?\"\n",
    "\n",
    "## 1.3) Example 1: Coin Flipping.\n",
    "- Example: You have two coins, one coin flips fairly, the other one is heads 80% of the time. What's the probability that the coin you just flipped is the unfair one if you got heads?\n",
    "    - \\begin{equation*}\n",
    "    P(Coin Unfair|Is Heads)=\\frac{P(Is Heads|Coin Unfair)*P(Coin Unfair)}{P(Is Heads)}\n",
    "    \\end{equation*}\n",
    "\n",
    "     \\begin{equation*}\n",
    "    P(Coin Unfair|Is Heads)=\\frac{P(Is Heads|Coin Unfair)*P(Coin Unfair)}{P(Is Heads|Coin Unfair)*P(Coin Unfair)+ P(Is Heads|Coin Not Unfair)*P(Coin Not Unfair)}\n",
    "    \\end{equation*}\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(Coin Unfair|Is Heads)=\\frac{0.8*0.5}{0.8*0.5+0.5*0.5}\n",
    "    \\end{equation*}\n",
    "\n",
    "## 1.4) Example 2: Spam Mail\n",
    "### 1.4.1) Let's say we have four emails that we know are spam or are not based on keywords we have below. How do we predict how likely a new email is spam?\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>ID</th> \n",
    "    <th>Terms in e-mail</th>\n",
    "    <th>Is it Spam</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Click win prize</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Click meeting setup meeting </td>\n",
    "    <td>No</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Prize free prize</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>Click prize free</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>Free setup meeting free</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  \n",
    " </table>\n",
    " \n",
    " The first four terms that we have labeled are what we train the network on (Training set), and the last term is what we're testing (test set).\n",
    "\n",
    "   \n",
    "### 1.4.2: Steps\n",
    "#### 1.4.2.i: Define S and NS events as the email being spam or not spam.\n",
    "- From the taining set, we get the following:\n",
    "\\begin{equation*}\n",
    "    P(S)=\\frac{3}{4}\n",
    "    \\end{equation*}\n",
    "\\begin{equation*}\n",
    "    P(NS)=\\frac{1}{4}\n",
    "    \\end{equation*}\n",
    "\n",
    "\n",
    "#### 1.4.2.ii: Set up to P(x|S), P(x|NS):\n",
    "- To calculate P(x|S), where x=(free, setup, meeting, free).\n",
    "- We'll need P(free|S), P(setup|S), P(meeting|S) based on training set.\n",
    "    - The ratio of the term showing up in all occurences of S in the training set.\n",
    "- However, with \"Free\" never appearing in NS, P(free|NS) is 0. Because of this and conditional independence assumption, P(x|NS)=0, and because this term is in the numerator of our equation, this will lead to P(NS|x) immediatley returning 0.\n",
    "    - To smooth this out, we'll start counting term occurence from 1 rather than 0\n",
    "        - Laplace smoothing.\n",
    "    - \\begin{equation*}\n",
    "    P(free|S)=\\frac{2+1}{9+6}=\\frac{3}{15}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    - 2: Occurences of term \"Free\" in S class.\n",
    "    - 1: Laplace smoothing constant.\n",
    "    - 9: Total term occurences in S class (spam).\n",
    "    - 6: One additional count per term from Laplace smoothing constant (Click, win, prize, meeting, setup, free).\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(free|NS)=\\frac{0+1}{4+6}=\\frac{1}{10}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    Similarly...\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(setup|S)=\\frac{0+1}{9+6}=\\frac{1}{15}\n",
    "    \\end{equation*}\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(setup|NS)=\\frac{1+1}{4+6}=\\frac{2}{10}\n",
    "    \\end{equation*}\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(meeting|S)=\\frac{0+1}{9+6}=\\frac{1}{15}\n",
    "    \\end{equation*}\n",
    "    \n",
    "     \\begin{equation*}\n",
    "    P(meeting|NS)=\\frac{2+1}{4+6}=\\frac{3}{10}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    Therefore...\n",
    "### 1.4.4: Calculate P(S|x), P(NS|x)\n",
    "\n",
    "    \\begin{equation*}\n",
    "    P(S|x)=\\frac{P(x|S)P(S)}{P(x)}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P(S|x)=\\frac{3}{4}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P(S|x)=\\frac{(P(free|S)*P(setup|S)*P(meeting|S)*P(free|S))*P(S)}{P(x|S)*P(S)+P(x|NS)*P(NS)}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P(S|X)=\\frac{(P(free|S)*P(setup|S)*P(meeting|S)*P(free|S))*P(S)}{(P(free|S)*P(setup|S)*P(meeting|S)*P(free|S)*P(S))+(P(free|NS)*P(setup|NS)*P(meeting|NS)*P(free|NS)*P(NS))}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P(S|X)=\\frac{\\frac{3}{15}*\\frac{1}{15}*\\frac{1}{15}*\\frac{3}{15}*\\frac{3}{4}}{\\frac{3}{15}*\\frac{1}{15}*\\frac{1}{15}*\\frac{3}{15}*\\frac{3}{4}+(\\frac{1}{10}*\\frac{2}{10}*\\frac{3}{10}*\\frac{1}{10}*\\frac{1}{4})}\n",
    "    \\end{equation*}\n",
    "    \n",
    "    \\begin{equation*}\n",
    "    P(S|X)=\\frac{8}{17}=1-P(NS|X)\n",
    "    \\end{equation*}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Implementing Naive Bayes Classifier to tell if story is happy or sad based on terms included.\n",
    "\n",
    "## 2.1) Importing required packages\n",
    "## 2.2) Set paths\n",
    "## 2.3) Take data and prepare it for parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook naive_bayes_work.ipynb to script\n",
      "[NbConvertApp] Writing 4884 bytes to naive_bayes_work.py\n"
     ]
    }
   ],
   "source": [
    "import naive_bayes_work #File with back-end equations\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join('data/')\n",
    "\n",
    "file=open(os.path.join(data_dir, 'words.csv'), 'r')\n",
    "reader = csv.reader(file)\n",
    "vocabulary = list(item[0] for item in reader)\n",
    "\n",
    "# Loading data into numpy arrays\n",
    "features_train = np.genfromtxt(os.path.join(data_dir, 'features_train.csv'), delimiter=',')\n",
    "labels_train = np.genfromtxt(os.path.join(data_dir, 'labels_train.csv'), delimiter=',')\n",
    "features_test = np.genfromtxt(os.path.join(data_dir, 'features_test.csv'), delimiter=',')\n",
    "labels_test = np.genfromtxt(os.path.join(data_dir, 'labels_test.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4) Set hyperparameter constants.\n",
    "### Laplace Smoothing Constant: What we weight each feature initially with in order to offset initial bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsc=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Get P(x|Yk) values\n",
    "### Create matrix of Joint distributions that indicate guess that each word indicates happy or sad story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihood=naive_bayes_work.NBFeatureGivenLabel(features_train, labels_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6) Get P(Yk) value\n",
    "### Scalar indicated chance that  story is sad based on all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior=naive_bayes_work.NBLabelPrior(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7) Calculate classified P(Yk|x) for each test article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_guess=naive_bayes_work.NBClassifier(likelihood, prior, features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8) Calculate the error\n",
    "### Based on your truth (labels_test) and what your classifier predicted (labels_guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1103448275862069\n"
     ]
    }
   ],
   "source": [
    "labelcount=labels_guess.shape[0]\n",
    "#Instantiate int for bad(matching)\n",
    "incorrect_match=0\n",
    "#Iterate through labelcount\n",
    "for i in range(0,labelcount):\n",
    "    ##If items are different, incorrect_match++\n",
    "    if (labels_guess[i]!=labels_test[i]):\n",
    "        incorrect_match=incorrect_match+1\n",
    "error = float(incorrect_match)/float(labelcount)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary\n",
    "## Binary Classification: Labels a set of data in one of two classes.\n",
    "## Multiclass Classification: Labels a set of data into 3+ Classes.\n",
    "## Multilabel Classification: Choosing which class-classifier to use (one of may binary or multiclass).\n",
    "## Named-Entity Recognition (NER): Subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories that use labels such as the names of persons, organizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
