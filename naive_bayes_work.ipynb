{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Helper functions\n",
    "\n",
    "## 1: Import useful things for math-related fuctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: NBFeatureGivenLabel function\n",
    "Takes a training set features_train and labels_train, the Laplace smoothing constant,<br>\n",
    "and tries to estimate the percent of time whether or not each word (V) indicates <br>\n",
    "that the article (1,2,...,n) is good or bad.\n",
    "\n",
    "### Variables:\n",
    "n: Number of articles.\n",
    "V: Number of words.\n",
    "\n",
    "\n",
    "### Inputs\n",
    "    # features_train - (n by V) numpy ndarray\n",
    "                #The entry features_train(i,j) is 1 if word j appears in the ith training passage and 0 otherwise.\n",
    "\t# labels_train - 1D numpy ndarray of length n\n",
    "    # lsc - laplace smoothing constant.\n",
    "    \n",
    "### Outputs\n",
    "    # P=2 by V array.\n",
    "        # Each row corrosponds to the probability that article is good (label=1) or bad (label=0) P(Xi|label) \n",
    "        # Each col corrosponds to a for each word Xi.\n",
    "        # Example of element:\n",
    "            #(0,3): P(word X_3 is present|label=bad)\n",
    "            # For word a and class label b, D(b,a) is the Lap-smoothed estimate.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBFeatureGivenLabel(features_train, labels_train, lsc):\n",
    "    features_train_shape=features_train.shape\n",
    "    V=features_train_shape[1]    #V: Number of words.\n",
    "    n=features_train_shape[0]    #n: Number of articles.\n",
    "    P = np.ones((2,V))           #P(X_1,X_2,....X_V present|Article is good/bad)\n",
    "    #Iterate over each word\n",
    "    for i in range(0,V):\n",
    "        #Initialize count for times term is found in sad stories.\n",
    "        found_given_bad=0\n",
    "        #Initialize count for times term is found in happy stories.\n",
    "        found_given_good=0\n",
    "        #Iterate over each story to fill constants\n",
    "        for j in range(0,n):\n",
    "            #If the word is found in the story\n",
    "            if(features_train[j,i]==1):\n",
    "                #If the word is found in a happy story, update that count.\n",
    "                #If the word is found in a sad story, update that count.\n",
    "                if(labels_train[j]==1):\n",
    "                    found_given_good=found_given_good+1\n",
    "                else:\n",
    "                    found_given_bad=found_given_bad+1\n",
    "                    \n",
    "        #Find the amount of stories that are happy.\n",
    "        y_ones=np.sum(labels_train)\n",
    "        #Find the amount of stories that are sad.\n",
    "        y_zeros=n-y_ones\n",
    "        #P(X_i|Y=0)\n",
    "        to_add_0=(found_given_bad+lsc)/(y_zeros+(lsc*y_zeros))\n",
    "        #P(X_i|Y=1)\n",
    "        to_add_1=(found_given_good+lsc)/(y_ones+(lsc*y_ones))    \n",
    "        P[0,i]=to_add_0\n",
    "        P[1,i]=to_add_1\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: NBLabelPrior function\n",
    "Finds the % chance of the story being bad, as a \"prior\".\n",
    "\n",
    "### Variables:\n",
    "n: Number of articles.\n",
    "V: Number of words.\n",
    "\n",
    "\n",
    "### Inputs\n",
    "    # labels_train - 1D numpy ndarray of length n\n",
    "    \n",
    "### Outputs\n",
    "    # % chance that story is bad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBLabelPrior(labels_train):\n",
    "    #Instantiate # of good stories found.\n",
    "    p = 0\n",
    "    #1)Loop through all of n, adding all 0's found to p\n",
    "    for i in range(0,labels_train.shape[0]):\n",
    "        if(labels_train[i]==0):\n",
    "            p=p+1\n",
    "    return float(p)/float(labels_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: NBClassifier function\n",
    "Takes the information you've gathered and tries to guess whether or not the story is bad based on the terms that are present.\n",
    "\n",
    "### Variables:\n",
    "n: Number of articles.\n",
    "V: Number of words.\n",
    "\n",
    "\n",
    "### Inputs\n",
    "    # likelihood: likelihood estimates. If the word is present in the story, tries to determine whether or not the story will be bad and good.\n",
    "\t# prior - Scalar detailing the blind guess for an sad story.\n",
    "    # features_test - laplace smoothing constant.\n",
    "    \n",
    "### Outputs\n",
    "    # Truth: 1D array of 1's or 0's, detailing guess of whether or not story is happy or sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBClassifier(likelihood, prior, features_test):\n",
    "    (m,V)=features_test.shape\n",
    "    guess_truth = np.ones(features_test.shape[0])\n",
    "    for i in range(0,m):\n",
    "        is_0=float(prior)\n",
    "        is_1=(1-prior)\n",
    "        for j in range(0,V):\n",
    "            #Extract from features_test each vocab word (j), and determine if word is there or not there.\n",
    "            j_word_here=features_test[i,j]\n",
    "            #If word is there, update the guess based on whether or not term is there\n",
    "            if (j_word_here):\n",
    "                is_0=is_0+float(likelihood[0,j])\n",
    "                is_1=is_1+float(likelihood[1,j])\n",
    "            else:\n",
    "                is_0=is_0+float(1-likelihood[0,j])\n",
    "                is_1=is_1+float(1-likelihood[1,j])\n",
    "        guess_truth[i]=(is_1>is_0)\n",
    "    return guess_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook naive_bayes_work.ipynb to script\n",
      "[NbConvertApp] Writing 4884 bytes to naive_bayes_work.py\n"
     ]
    }
   ],
   "source": [
    "#Below line used to convert jupyter file to .py file\n",
    "#Comment  out below line to speed up import of file.\n",
    "!jupyter nbconvert --to script naive_bayes_work.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
